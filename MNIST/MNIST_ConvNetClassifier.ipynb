{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- Setup \n",
    "    - Dataset and DataLoader\n",
    "    - Training loop\n",
    "    - Model architecture\n",
    "- Model training\n",
    "- Model Testing\n",
    "- General CNN Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-lr-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, Tensor\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import display_tensor_as_image, set_optimal_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device.type = 'cpu'\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'{device.type = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mnist = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "mnist_loader = DataLoader(dataset=mnist, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=112x112>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tensor_as_image(mnist[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def train_one_epoch(model: nn.Module, dataloader: DataLoader, loss_fn: Callable[[Tensor, Tensor], Tensor], optimizer: torch.optim.Optimizer) -> float:\n",
    "    epoch_running_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "\n",
    "        inputs = inputs.to(device) #  move to GPU if available\n",
    "        labels = labels.to(device) #  move to GPU if available\n",
    "\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        # compute loss and propogate gradients\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # update metainformation\n",
    "        epoch_running_loss += loss.item()\n",
    "\n",
    "    epoch_avg_loss = epoch_running_loss / len(dataloader)\n",
    "    return epoch_avg_loss\n",
    "\n",
    "\n",
    "def train_n_epochs(n: int, model: nn.Module, dataloader: DataLoader, loss_fn: Callable[[Tensor, Tensor], Tensor], optimizer: torch.optim.Optimizer) -> None:\n",
    "    for i in range(n):\n",
    "        epoch_avg_loss = train_one_epoch(model, dataloader, loss_fn, optimizer)\n",
    "        print(f'epoch {i}: {epoch_avg_loss = }')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Convolutional Neural Network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        in_channels = 1 # grayscale image\n",
    "        \n",
    "        conv_1_n_kernels = 6\n",
    "        conv_1_kernel_size = 5\n",
    "        conv_1_padding = int((conv_1_kernel_size - 1) / 2) # this is the formula to preserve size assuming a stride of 1\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=conv_1_n_kernels,\n",
    "                      kernel_size=conv_1_kernel_size,\n",
    "                      stride=1,\n",
    "                      padding=conv_1_padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # this downsamples the input by factor of 2\n",
    "        )\n",
    "\n",
    "        fully_connected_1_in = int(28 /  2) ** 2 # 28*28 gets downsampled to 14*14\n",
    "        fully_connected_1_out = 100\n",
    "        self.fully_connected_1 = nn.Sequential(\n",
    "            nn.Linear(fully_connected_1_in * conv_1_n_kernels, fully_connected_1_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fully_connected_2 = nn.Sequential(\n",
    "            nn.Linear(fully_connected_1_out, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = x.flatten(start_dim=1) # [batch_size, 14, 14] => [batch_size, 14*14]\n",
    "        x = self.fully_connected_1(x)\n",
    "        x = self.fully_connected_2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lr=.001, params=cnn.parameters())\n",
    "\n",
    "# set_optimal_learning_rate(cnn, optimizer, loss_fn, mnist_loader, device, end_lr=.01, num_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: epoch_avg_loss = 0.3165370847886877\n",
      "epoch 1: epoch_avg_loss = 0.10736762019577248\n",
      "epoch 2: epoch_avg_loss = 0.07240611262994805\n",
      "epoch 3: epoch_avg_loss = 0.0548827539633479\n",
      "epoch 4: epoch_avg_loss = 0.04335699439322306\n",
      "epoch 5: epoch_avg_loss = 0.036311111557654846\n",
      "epoch 6: epoch_avg_loss = 0.02962964236246559\n",
      "epoch 7: epoch_avg_loss = 0.02417637694297286\n",
      "epoch 8: epoch_avg_loss = 0.021054732668959946\n",
      "epoch 9: epoch_avg_loss = 0.01798543394639726\n",
      "epoch 10: epoch_avg_loss = 0.015015486319418942\n",
      "epoch 11: epoch_avg_loss = 0.013329268066780463\n",
      "epoch 12: epoch_avg_loss = 0.011218645271877988\n",
      "epoch 13: epoch_avg_loss = 0.008903738465816985\n",
      "epoch 14: epoch_avg_loss = 0.009002252518585321\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "train_n_epochs(n_epochs, cnn, mnist_loader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "mnist_test_loader = DataLoader(dataset=mnist_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the cnn on the MNIST test images: 98.64%\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in mnist_test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = cnn(inputs)\n",
    "\n",
    "        _, predictions = torch.max(logits, 1)\n",
    "\n",
    "        # predictions.shape = [64]\n",
    "        # labels.shape = [64]\n",
    "\n",
    "        total += len(inputs)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the cnn on the MNIST test images: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General CNN Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the kernel really doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel.weight = Parameter containing:\n",
      "tensor([[[[-0.3305, -0.2630, -0.1428],\n",
      "          [-0.3154, -0.0189, -0.2933],\n",
      "          [ 0.1812,  0.1625,  0.0629]]]], requires_grad=True) \n",
      "\n",
      "input_data = tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "kernel = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=1)\n",
    "print(f'{kernel.weight = }', '\\n')\n",
    "\n",
    "input_data = torch.ones(1, 5, 5)\n",
    "print(f'{input_data = }', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let kernel size be `x * x`. \n",
    "\n",
    "The kernel slides across the input data, looking at every chunk of size `x * x` in the input data.\n",
    "\n",
    "For each chunk, the kernel outputs the `sum(element_wise_multiplication(kernel.weight, current_chunk_of_input)) + kernel.bias`\n",
    "\n",
    "---\n",
    "\n",
    "This calculation is essentially a unique type of the dot product. It calculates the similarity between the input chunk and the kernel.\n",
    "\n",
    "The feature map (aka output of kernel), is of the same shape as the input when the padding is correct. Each \"pixel\" in the feature map represents this calculation against the `current_chunk_of_input` data. Where `current_chunk_of_input` is a piece of the input data, that is the same size as the kernel. The kernel slides across each possible chunk of the size of the kernel in the input (when the stride is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map = tensor([[[ 0.1718,  0.0377,  0.0377,  0.0377,  0.2680],\n",
      "         [-0.2339, -0.6985, -0.6985, -0.6985, -0.3255],\n",
      "         [-0.2339, -0.6985, -0.6985, -0.6985, -0.3255],\n",
      "         [-0.2339, -0.6985, -0.6985, -0.6985, -0.3255],\n",
      "         [-0.4593, -1.1052, -1.1052, -1.1052, -0.6692]]],\n",
      "       grad_fn=<SqueezeBackward1>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_map = kernel(input_data)\n",
    "print(f'{feature_map = }', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the top left value of the `feature_map` is `.01718` (from `feature_map[0][0]`).\n",
    "\n",
    "That means that the dot product like calculation of `sum(element_wise_multiplication(kernel.weight, current_chunk_of_input)) + kernel.bias` for the `first_chunk_of_input` and `kernel` should be `.01718`. Let's verify that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_chunk_of_input = tensor([[[1., 1.],\n",
      "         [1., 1.]]]) \n",
      "\n",
      "padded first_chunk_of_input = tensor([[[0., 0., 0.],\n",
      "         [0., 1., 1.],\n",
      "         [0., 1., 1.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "first_chunk_of_input = input_data[:2, :2, :2] # the top left chunk of the input_data. because our kernel uses padding, we need to pad this with 0's.\n",
    "print(f'{first_chunk_of_input = }', '\\n')\n",
    "\n",
    "# pad tuple: (left, right, top, bottom)\n",
    "first_chunk_of_input = F.pad(first_chunk_of_input, (1, 0, 1, 0), 'constant', 0)\n",
    "print(f'padded {first_chunk_of_input = }', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_wise_mulitplication = tensor([[[[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0189, -0.2933],\n",
      "          [ 0.0000,  0.1625,  0.0629]]]], grad_fn=<MulBackward0>) \n",
      "\n",
      "output = tensor([0.1718], grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "element_wise_mulitplication = kernel.weight * first_chunk_of_input\n",
    "print(f'{element_wise_mulitplication = }', '\\n')\n",
    "\n",
    "output = element_wise_mulitplication.sum() + kernel.bias\n",
    "print(f'{output = }', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
